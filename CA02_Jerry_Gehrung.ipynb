{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_Jerry_Gehrung.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI604XfX2QTG"
      },
      "source": [
        "# Spam eMail Detection using Naive Bayes Classification Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr"
      },
      "source": [
        "# import packages\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKx84wb5qy4r",
        "outputId": "4dd7355b-5832-4604-ed83-9939e7708f40"
      },
      "source": [
        "# mount drive to import folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjVk9nrgmhAM"
      },
      "source": [
        "# import folders\n",
        "train_mails = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\n",
        "test_mails = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcf9oc5v3UVD"
      },
      "source": [
        "## Cleaning and Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "def create_Dictionary(root_dir): # primary directory storing all txt files\n",
        "  all_words = [] # blank list for storing all words from both folders\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for email in emails:\n",
        "    with open(email) as e: # opens and returns file\n",
        "      for line in e:\n",
        "        words = line.split() # divides text into separate words\n",
        "        all_words += words # adds all new words to 'words'\n",
        "  words_dictionary = Counter(all_words) # creates dictionary that keeps track of how many times equivalent values are added\n",
        "  remove_list = list(words_dictionary) # transform words_dictionary into list of words to remove\n",
        "\n",
        "  for item in remove_list:\n",
        "    if item.isalpha() == False: # checks if characters in the text are letters\n",
        "      del words_dictionary[item] # removes values that are not letters\n",
        "    elif len(item) == 1: # checks for single-character words\n",
        "      del words_dictionary[item] # deletes values that are single-character\n",
        "  words_dictionary = words_dictionary.most_common(3000) # keeps only the 3,000 most common words\n",
        "  return words_dictionary # prints the 3,000 most common words"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGPF_JZh2-Vy"
      },
      "source": [
        "## Extracting features and corresponding label matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "def derive_features(mail_master):\n",
        "  mail_files = [os.path.join(mail_master,mf) for mf in os.listdir(mail_master)]\n",
        "  features_matrix = np.zeros((len(mail_files),3000)) # creates array filled with zeroes\n",
        "  train_labels = np.zeros(len(mail_files))\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "  for fil in mail_files:\n",
        "    with open(fil) as mf: # opens and returns file\n",
        "      for i, line in enumerate(mf): # adds a counter to mf and returns it in a form of enumerate object\n",
        "        if i ==2:\n",
        "          words = line.split() # divides text into separate words\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(words_dictionary): # adds a counter to the dictionary and returns it in a form of enumerate object\n",
        "              if d[0] == word: \n",
        "                wordID = i\n",
        "                features_matrix[docID,wordID] = words.count(word) # checks for words that also exist in the dictionary and counts number of occurences\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"): # 'spmsg' represents spam\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1 # adds 1 for each occurence of spam\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels                "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-b0fe9w2rxM"
      },
      "source": [
        "## Training and predicting with sklearn Naive Bayes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "24fa3c10-5082-405f-c7ac-e8a939687b36"
      },
      "source": [
        "words_dictionary = create_Dictionary(train_mails) # assigns training data folder to dictionary\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = derive_features(train_mails) # assigns training data folder to the features matrix\n",
        "test_features_matrix, test_labels = derive_features(test_mails) # assigns testing data folder to the test features matrix\n",
        "\n",
        "model = GaussianNB() # assigns Gaussian as the model type\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels) # uses training data to train the model\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix) # uses testing data for the prediction model"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG8Nuf2n3oEv"
      },
      "source": [
        "## Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMKF4xQR3mmt",
        "outputId": "9b780f7e-8ea7-425d-9c4f-d197025ca6eb"
      },
      "source": [
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}